{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blanco-herrero/Interviews/blob/main/TOPIC_MODELLING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikL-0IsjGAz"
      },
      "source": [
        "# Importing modules\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "from gensim import matutils\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "os.chdir('..')\n",
        "from glob import glob\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.sentiment import vader\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmMus78kjz3l"
      },
      "source": [
        "# Upload the file\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM2JCy9QkP3T"
      },
      "source": [
        "# Read data into an object\n",
        "entrevistas_df = pd.read_csv(\"Entrevistas.txt\", header= None, sep='\\t')\n",
        "entrevistas = entrevistas_df.T.squeeze()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsEuOc4klPCI"
      },
      "source": [
        "# Print out the first rows of the file\n",
        "entrevistas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbNTZsTclbFT"
      },
      "source": [
        "# Remove the columns (if the case)\n",
        "# trans = trans.drop(columns=['id', 'location', 'language'], axis=1).sample(100)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "(lambda x: re.sub('[-–_–,:/.¡!¿?...]','', x))"
      ],
      "metadata": {
        "id": "Vc-o9LkcxU5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h0pVTVfmebp"
      },
      "source": [
        "# Remove punctuation\n",
        "entrevistas = entrevistas.map(lambda x: re.sub('[-–_–,:/.¡!¿?...]','', x))\n",
        "entrevistas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXEPndF2m60p"
      },
      "source": [
        "# Convert uppercase to lowercase\n",
        "entrevistas.map(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF_58njOoh12"
      },
      "source": [
        "# PREPARE DATA FOR LDA ANALYSIS\n",
        "\n",
        "# Generate the stop words\n",
        "stop_words = stopwords.words('spanish')\n",
        "\n",
        "stop_words.extend(['http', 'https', 'tco', 'httpstco', 'co', 'pa', 'si', \n",
        "                   'rt', 'etc', 'ect', 'tco', 'xa0', 't', 'si', 'q', 'd', 'lo', 'ell', 'call', \n",
        "                   'pkly', 'onde', 'pese', 'tb', '000', 'dos', 'tres', 'cia', 'vez', 'ves', \n",
        "                   'mira', 'tan', 'tal', 'dar', 'da', 'das', 'dan', 'uma', 'va', 'van', 'ser',\n",
        "                   'ahí', 'ahi', 'tras', 'detrás', 'detras', 'creo', 'hecho', 'ejemplo', 'demas', \n",
        "                   'demás', 'ademas', 'además', 'resto', 'pensar', 'sino', 'decir', 'lado', \n",
        "                   'parece', 'piensa', 'pase', 'pesar', 'entender', 'alguien', 'dicho', 'supuesto', \n",
        "                   'alli', 'allí', 'aun', 'cualquier', 'cuestión', 'contenido', 'contenidos', \n",
        "                   'simplemente', 'símplemente', 'habia', 'parte', 'acaso', 'biden', 'toda', 'todas',\n",
        "                   'gracias', 'despues', 'después', 'of', 'and', 'his', 'the', 'cada', 'to', \n",
        "                   'in', 'ello', 'quiere', 'buenas', 'ningún', 'día', 'pocos', 'cómo', 'como', 'puede', \n",
        "                   'pone', 'mientras', 'garcimoreno', 'cabrita', 'veintimillapier', 'qls', 'colocolo', \n",
        "                   'gyzytqqtbm', 'sólo', 'solo', 'ahora', 'frente', 'hacen', 'hace', 'bien', 'años', \n",
        "                   'nueva', 'luego', 'así', 'asi','claro', 'ver', 'debería', 'video', 'estan', 'mas', \n",
        "                   'menos', 'hacer', 'solo', 'pues', 'incluso', 'meses', 'vale', 'dia', 'evidentemente', \n",
        "                   'tambien', 'entonces','bueno', 'gente', 'discurso', 'aqui', 'veces', 'digo', 'vamos', \n",
        "                   'momento', 'forma', 'cosas', 'tipo', 'siempre', 'caso', 'persona', 'personas', 'puedes', \n",
        "                   'quiza', 'pasa', 'tema', 'alguna', 'algunas', 'manera', 'veo', 'final', 'nunca', 'muchas', \n",
        "                   'veces', 'igual', 'quizas', 'dice', 'tener', 'hacia', 'digamos', 'bastante', 'mucha', \n",
        "                   'tampoco', 'tambien', 'mismo', 'decia', 'voy', 'cosa', 'ambito', 'aunque', 'algun',\n",
        "                   'puedo', 'haber', 'quiero', 'pueden', 'mejor', 'lleva', 'dicen', 'depende', 'sido',\n",
        "                   'general'])\n",
        "\n",
        "len(stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "id": "kdJVIKPBacE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndl0f25Po4Tb"
      },
      "source": [
        "#spanish_stopwords.append('')\n",
        "#spanish_stopwords.remove('')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYPvX02zpHOx"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        \n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) \n",
        "             if word not in stop_words] for doc in texts]  "
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euAqi_LfpO9E"
      },
      "source": [
        "data = entrevistas.values.tolist()\n",
        "data_words = list(sent_to_words(data))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsMjg_aRpd1o"
      },
      "source": [
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxDNIz_Cu8gf"
      },
      "source": [
        "print(data_words[:1][0][:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmjTMBY5qacq"
      },
      "source": [
        "import gensim.corpora as corpora\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3kO9-jWqeLt"
      },
      "source": [
        "# LDA model training\n",
        "\n",
        "from pprint import pprint\n",
        "# Number of topics\n",
        "num_topics = 4\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = models.ldamodel.LdaModel(num_topics=num_topics, corpus=corpus, id2word=id2word, random_state=None, \n",
        "                                     per_word_topics=True, alpha='auto', passes=2)\n",
        "lda_model.print_topics(num_words=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_dlMxCrq8-w"
      },
      "source": [
        "# Print the keywords of the topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]\n",
        "lda_model.print_topics(num_words=40)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlkkpLjEtRq-"
      },
      "source": [
        "# Analyze LDA model results\n",
        "!pip install pyLDAvis\n",
        "!python -m pip install -U pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import pickle "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnPI5D6nt542"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(num_topics))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zMoeYi2uF23"
      },
      "source": [
        "# This is a bit time consuming - make the if statement True\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o7XX79Cys1F"
      },
      "source": [
        "# Load the prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "    pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "LDAvis_prepared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnFTh0u7S541"
      },
      "source": [
        "# Take a look to the texts and their topics\n",
        "topics = pd.DataFrame([dict(lda_model.get_document_topics(doc, minimum_probability=0.1))\n",
        "                      for doc in corpus])\n",
        "meta = entrevistas.iloc[entrevistas.index].drop(columns=[0]).reset_index(drop=True)\n",
        "tpd = pd.concat([meta, topics], axis=1)\n",
        "tpd [:50]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}